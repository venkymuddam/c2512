I am doing a project LLM model, I will give some description of our project, then please give me the hardcoded code for my project because before starting the original code i should dod the UI hardcoded to understand it beter.

Project: The user will enter the message in the react to store that inside the RAG, so after user enter the message from react it will pass the message string to the fastapi(post method) from react. From fastapi it will receive that message and fatsapis will transfer that string to the RAG + OLLAMA systems which will take that string from fastapi and store that data inside the rag for the future responce generation(by ollama LLM) to query based on this stored data. and after rag stored the data succeessfully or failed it will return a success or fail message status of storing to the fastapi and the fastapi will be receiving that message from that and it will return that message to the react and react will display that message. And second user will enter a his query in react  frontend then query will go the python fastapi by using fastapi calling in react so it is like creating a new entity(POST), so python fastapi will be receiving that query string and it will pass it to the another container which will contain RAG database system and Ollama system, python dont know what happening inside that it will just passing the required  information like query to that container so that will return generated output(generated responce by ollama LLM) for the query to the fatapi back. And python fastapi will take that responce from that system and return that generated output back to the react so that react will print the generated output along with entered query in the react frontend. In fastapi it dont know how and what is happen inside the rag + ollama system it will just pass the string with the help of functions(which will be actually implemented inside the rag + ollama system) of rag + ollama system and it will receive the returned responce from that sysem. Multiple users can query at same time. So the container i mentioned before(RAG, ollama) it contains like rag will store the text message (which is entered in react frontend and goes to RAG through fastAPI call and no chunking is required inside RAG) in it which had some information in that for example it had content like details of all the employees and their login and logout details so if user enter "who are logged in at 5pm" then by using this query it will use RAG and Ollama are used to generate this answer so it goes like that. So python fastapi, rag, Ollama these contained inside the docker. 




                                ------------------------------Project Requirements-------------------------------

1. React Frontend (Outside Docker Container)
        => PURPOSE: Handles user interactions like sending user texts and query searching, and communicates with FastAPI backend.

        => Features:
            - Enters User Text in RAG : Sends text to FastAPI for storing in RAG.

            - Query RAG : Sends query to FastAPI to retrieve a generated response.

            - Styling & UI Components : CSS for styling and React components for UI.

        => DEPENDENCIES: 
            - Axios : Used for API calls to FastAPI.

            - React Router(react-router-dom) : Enables component routing.

            - React Hooks(useState, useEffect) : Manages state and side effects.

            - Material UI / Bootstrap(optional) : For UI styling.

        => DATA-FLOW:
            1. User enters text : React sends POST request to FastAPI.
             - It Waits for confirmation from FastAPI before proceeding.

            2. User submits a query : React sends POST request to FastAPI.
             - It Waits for generated response before displaying it.

            3. Receives responses from FastAPI  in JSON format and updates UI.

        => WAITING-REQUIREMENTS:
            - Waits for confirmation after storing user text.
            - Waits for generated output from RAG before displaying the response.

------

2. FastAPI Backend (Inside Docker Container)
        => Purpose : It acts as a bridge between React frontend and RAG + Ollama container.

        => Features :
            - Receives and passes user text in RAG.

            - Receives queries, forwards them to RAG & Ollama, and returns responses.

            - Manages API endpoints for storing and querying data.

            - Handles multiple user requests concurrently by using async and await features.

        => Dependencies:
            - FastAPI : Framework for creating RESTful APIs.

            - Uvicorn : ASGI server to run FastAPI.

            - Pydantic : Validates incoming request data.

            - Ollama Python SDK : Communicates with Ollama for generating the responses.

        => Data Flow:
            1. Receives API request from React (either to store text or query).

            2. Forwards user text to RAG for storage.
             - Waits for confirmation before responding to React.

            3. Forwards queries to RAG & Ollama.
             - Waits for response before sending it to React.

        => Waiting-Requirements:
            - Waits for RAG confirmation before notifying React about successful text storage.
            - Waits for response from RAG + Ollama before sending it back to React.

------

3. RAG & Ollama System (Inside Docker Container)
    => PURPOSE : Handles text storage, vector embedding, and response generation.

    => FEATURES :
        - Stores user-entered text as vectors for retrieval.

        - Processes queries using embedded data and LLM model.

        - Returns generated responses based on stored data.

    => DEPENDENCIES :
        - Embedding (all_MiniLM_L6_V2) : enables similarity search in vector db.

        - FAISS / ChromaDB : It will Converts text into vector embeddings and stores them.

        - LangChain : It will Manages interaction between RAG and Ollama.

        - Ollama LLM : It will It will Generates responses based on vectorized data.

        - NumPy / SciPy : It will Handles mathematical computations in vector search.

    => DATA-FLOW :
        1. Receives user-entered text from FastAPI and embeds it into vectors.
            - Waits for embedding confirmation before notifying FastAPI.

        2. Receives query from FastAPI, searches stored vectors, and passes relevant data to Ollama.
             - Waits for generated response before sending it back to FastAPI.

    => WAITING-REQUIREMENTS :
        - Waits for vector embedding process to complete before confirming storage.

        - Waits for Ollama to generate a response before sending it to FastAPI.

------

4. Docker (Containerized Deployment)
    => PURPOSE : Manages isolated environments for FastAPI, RAG, and Ollama.

    => FEATURES :
        - Encapsulates FastAPI, RAG, and Ollama into separate containers.

        - Ensures consistent execution across different systems.

        - Allows multiple users to interact simultaneously without conflicts. // implement kubernetes to implement this.

    => DEPENDENCIES :
        - Docker Engine : Runs the containers.

        - Docker Compose : Manages multiple containers and networking.

        - Python (for FastAPI) : Runs FastAPI inside the container.

        - FAISS / ChromaDB Image : Handles vector storage inside the RAG container and Runs the LLM model inside a separate container.

    => DATA-FLOW :
        1. React sends request to FastAPI inside its container.

        2. FastAPI forwards the request to the RAG + Ollama container.

        3. RAG will stores/retrieves text and generates responses with Ollama.

        4. FastAPI sends response back to React.

    => WAITING-REQUIREMENTS :
        - FastAPI container must wait for RAG & Ollama container to initialize.

        - RAG container must finish loading stored data before processing queries.

        - Ollama must finish model initialization before generating responses.

------


So these are some details now give me the hard coded UI which only do the control flow of the coding like only passing the the string and 
printing the returned responce that it it will does no care of the inside functionality of the code it just hard coded to understand the 
flow of the design like which one is taking which one and which one is returning which one not doing any logical part or any algorithms or 
not storing inside rag just taking it and just returning the success message or some simple generated responce message(without implementing it 
actually).please give me the hard coded for react UI(hardcoded) part, fastapi UI(hardcoded) part, rag + ollama part.


/////////////////////////////////////////////////////////////////////////////////////////////////////////////////


/*                                                                   CLEAN CODE.....

const area = function (rad) {
    return Math.PI * rad * rad 

}

const curcumference = function (rad) {
    return 2 * Math.PI * rad
}

const calculate = function (logic) {
    const result=[]
    for(let i=0; i<radius_arr.length; i++){
        result.push(logic(radius_arr[i]))
    }
    return result
}


const radius_arr=[2,3,4,5]
console.log(calculate(area))
console.log(calculate(curcumference))

*/
////////////////////////////////////////////












/*                                                           MAP,FILTER,REDUCE...

function doubleFunction(x) {
    return x.toString(2)
}
function oddNumbers(x) {
    if (x % 2 == 1) {
        return x
    }
}

const arr1 = [1,2,3,4,5]

const arr2 = arr1.map(doubleFunction)
console.log(arr2)

const arr3 = arr1.filter(oddNumbers)
console.log(arr3)
*/
////////////////////////////////////////////











/*                                                         PROMISES IN JAVASCRIPT 

const cart="" 
const promise = createOrder(cart);


promise.then(function (orderId){
    console.log(",orderId,");
    return orderId;
})
.then(function (orderId){
    console.log(orderId);
})
.catch(function (err){
    console.log(err.message);
        
    }
);


function createOrder(cart) {

    const pr = new Promise(function (resolve,reject) {

        if (validateCart(cart)==false) {
            const err = new Error("Cart in invalid!");
            reject(err);
        }
        const orderId = 12345;
        if (validateCart(cart)==true){
            resolve(orderId);
        }
    });
    return pr;
}

function validateCart(cart){
    return true;
}
*/
/////////////////////////////////////////////









/*
   // promise.all,         wait until last seconds and print if all prmises are resolve...if any one promise is failed then it will print entire promises as failed.
   // promise.allSettled,  wait until last seconds and print if all prmises are resolve...it will print all either resolve or reject 
   // promise.race,        any promise either resolve or reject...first come first print..print immediatly after required promise found 
   // promise.any,         only resolved promise..first come first print...if all are rejected then print all as failed..print immediatly after required promise found 

const p1 = new Promise((resolve,reject) =>{
    //setTimeout(()=>{resolve("this is p1")},5000);
    setTimeout(()=>{reject("FAILED p1")},5000);
});

const p2 = new Promise((resolve,reject) =>{
    //setTimeout(()=>{resolve("this is p2")},3000);
    setTimeout(()=>{reject("FAILED p2")},3000);
});

const p3 = new Promise((resolve,reject) =>{
    //setTimeout(()=>{resolve("this is p3")},1000);
    setTimeout(()=>{reject("FAILED p3")},1000);
});


Promise.any([p1,p2,p3])
.then((res) =>{
    console.log(res);
})
.catch((err) =>{
    console.error(err)
});
*/
////////////////////////////////////////////////////










     // in normal promises it will execute the next line immediatly, but by using the "async" and "await" we can wait for some time to execute the next line after the promise was being called.


//                                                                    async
/*
async function venky1() { // async function always returns a promise
    return "javascript"
}

const dataa = venky1();
console.log(dataa);

dataa.then((res) => {
    console.log(res);
})
*/

//                                                              async along with await

const p1 = new Promise((resolve,reject) => {
    setTimeout(() => {
        resolve("Resolved Promise 1")},5000);
});


const p2 = new Promise((resolve,reject) => {
    setTimeout(() => {
        resolve("Resolved Promise 2")},1000);
});

async function venky2(){
// all the promises calls inside the async calls parallelly and to better understanding the promises calls will pushed into the QUEUE and after the completing the first pushed item only the the second pushed item will be exwcuted.
// but actualyy the javascript enjine works differently i wriiten this QUEUE example for easy understanding..
// actually the call stack will be eliminated the async function for sometime while calling the await p1.. and again callstach will call the async function again from next line of the promise, after time over in await promise.


    console.log("start");
    const storeData1 = await p1;  // await is always written inside the async function // await will call the promise as same as p1.then()
    console.log("p1 await called"); 
    console.log(storeData1); 

    const storeData2 = await p2;
    console.log("p2 await called");
    console.log(storeData2);
};

venky2().catch(() => {
    console.log("Failed");
});

//////////////////////////////////////////////////






/*                                                                    USE STRICT
"use strict";

console.log(this);

function x() {
    console.log(this);
}
x();

const z = {
    a: "bhashyam",
    b: function () {
        console.log(this.a);
    }
}


const y = {
    a: "venky",
    b: function () {
        console.log(this);
    }
}

z.b.call(y);


const e = {
    a:"muddam",
    b: () => {
        console.log(this)
    }
}

e.b();
*/
